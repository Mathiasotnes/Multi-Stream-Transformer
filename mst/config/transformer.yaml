##############################################################################
## transformer.yaml                                                         ##
## ----------------                                                         ##
## Configuration file for model, dataset, and training parameters for       ##
## benchmark transformer model.                                             ##
##                                                                          ##
## ------------------------------------------------------------------------ ##
## Author:   Mathias Otnes                                                  ##
## year:     2024                                                           ##
##############################################################################

model:
  d_model: 512
  num_heads: 8
  num_layers: 6
  d_ff: 2048
  dropout_rate: 0.1

dataset:
  name: openwebtext
  tokenizer_name: gpt2
  max_seq_length: 64
  batch_size: 8

training:
  num_epochs: 10
  learning_rate: 0.01
