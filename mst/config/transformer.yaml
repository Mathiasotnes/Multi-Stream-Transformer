##############################################################################
## transformer.yaml                                                         ##
## ----------------                                                         ##
## Configuration file for model, dataset, and training parameters for       ##
## benchmark transformer model.                                             ##
##                                                                          ##
## ------------------------------------------------------------------------ ##
## Author:   Mathias Otnes                                                  ##
## year:     2024                                                           ##
##############################################################################

model:
  d_model: 64
  num_heads: 4
  num_layers: 6
  d_ff: 512
  dropout_rate: 0.1

dataset:
  name: openwebtext
  tokenizer_name: gpt2
  max_seq_length: 64
  batch_size: 8

training:
  tokens: 300000
  learning_rate: 0.01
  eval_interval: 100
  eval_iters: 100
