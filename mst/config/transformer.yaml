##############################################################################
## transformer.yaml                                                         ##
## ----------------                                                         ##
## Configuration file for model, dataset, and training parameters for       ##
## benchmark transformer model.                                             ##
##                                                                          ##
## ------------------------------------------------------------------------ ##
## Author:   Mathias Otnes                                                  ##
## year:     2024                                                           ##
##############################################################################

model:
  d_model: 128
  num_heads: 8
  num_layers: 8
  d_ff: 512
  dropout_rate: 0.1

dataset:
  name: ptb
  tokenizer_name: gpt2
  max_seq_length: 128
  batch_size: 64

training:
  tokens: 21887860
  learning_rate: 0.002
  eval_interval_tokens: 131072
  eval_tokens: 8192
  model_name: 'transformer'
  checkpoint_interval_tokens: 1048576
  checkpoint_dir: './mst/models'
  log_dir: './mst/runs'
