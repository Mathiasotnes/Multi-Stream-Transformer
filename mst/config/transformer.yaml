##############################################################################
## transformer.yaml                                                         ##
## ----------------                                                         ##
## Configuration file for model, dataset, and training parameters for       ##
## benchmark transformer model.                                             ##
##                                                                          ##
## ------------------------------------------------------------------------ ##
## Author:   Mathias Otnes                                                  ##
## year:     2024                                                           ##
##############################################################################

model:
  d_model: 512
  num_heads: 8
  num_layers: 4
  d_ff: 256
  dropout_rate: 0.1

dataset:
  name: openwebtext
  tokenizer_name: gpt2
  max_seq_length: 128
  batch_size: 32

training:
  tokens: 26214400
  learning_rate: 0.01
  eval_interval_tokens: 16384
  eval_tokens: 1024
  model_name: 'transformer'
  checkpoint_interval_tokens: 50000
  checkpoint_dir: './mst/models'
  log_dir: './mst/runs'